{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a844d236",
   "metadata": {},
   "source": [
    "\n",
    "# High-Speed Clustering Grid Search with Auto-Save\n",
    "\n",
    "**Algorithms**: K-Means, K-Medoids (CLARA), and DBSCAN.\n",
    "**Speed Optimization**: Uses sub-sampling for metric calculation and parallel processing.\n",
    "**Safety**: Saves results to CSV after every iteration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd9475c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Libraries loaded. Metrics will be calculated on samples for speed.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## 0. Imports and Setup\n",
    "\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "\n",
    "# Algorithms\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration\n",
    "DATA_PATH = \"../../data/processed/Fire/final.csv\"\n",
    "RESULTS_FILE = \"clustering_grid_results.csv\"\n",
    "SAMPLE_SIZE_METRICS = 5000  # <--- HUGE SPEEDUP: Calculate metrics on this many points\n",
    "RANDOM_STATE = 42\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "154a9d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data ready: (104372, 15)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## 1. Data Loading & Preprocessing\n",
    "\n",
    "def prepare_clustering_data(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "    # Remove non-feature columns (adjust names if they differ in your file)\n",
    "    exclude = [\"fire\", \"latitude\", \"longitude\", \"Lat\", \"Lon\"]\n",
    "    cols = [c for c in df.columns if c not in exclude]\n",
    "    \n",
    "    X = df[cols].values\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    return X_scaled\n",
    "\n",
    "X_scaled = prepare_clustering_data(DATA_PATH)\n",
    "print(f\"Data ready: {X_scaled.shape}\")\n",
    "\n",
    "# Pre-calculate a fixed sample for consistent evaluation speed\n",
    "X_eval_sample = resample(X_scaled, n_samples=min(SAMPLE_SIZE_METRICS, len(X_scaled)), \n",
    "                         random_state=RANDOM_STATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63eadcf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## 2. Robust Clustering Grid Search\n",
    "\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "def run_clustering_search(model_class, param_grid, model_name, X_train, X_eval, output_file):\n",
    "    print(f\"\\n\" + \"=\"*60)\n",
    "    print(f\"Starting Search: {model_name}\")\n",
    "    \n",
    "    grid = list(ParameterGrid(param_grid))\n",
    "    \n",
    "    # Initialize CSV if not exists\n",
    "    if not os.path.exists(output_file):\n",
    "        pd.DataFrame(columns=['timestamp', 'model', 'params', 'silhouette', 'db_index', 'ch_score', 'n_clusters', 'time']).to_csv(output_file, index=False)\n",
    "\n",
    "    for i, params in enumerate(grid):\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            # 1. Initialize and Fit\n",
    "            model = model_class(**params)\n",
    "            \n",
    "            # For CLARA/KMedoids, we often fit on a sample but predict on the eval set\n",
    "            # For KMeans/DBSCAN, we fit on the whole or a large sample\n",
    "            labels = model.fit_predict(X_train)\n",
    "            \n",
    "            # 2. Evaluation (On the fixed evaluation sample for speed)\n",
    "            # We must predict labels for the evaluation sample specifically\n",
    "            if hasattr(model, 'predict'):\n",
    "                eval_labels = model.predict(X_eval)\n",
    "            else:\n",
    "                # For DBSCAN, predict doesn't exist, we must fit on the eval sample or find nearest\n",
    "                eval_labels = model.fit_predict(X_eval)\n",
    "\n",
    "            n_clusters = len(set(eval_labels)) - (1 if -1 in eval_labels else 0)\n",
    "            \n",
    "            # Metrics (only if we have more than 1 cluster and not all noise)\n",
    "            if n_clusters > 1:\n",
    "                sil = silhouette_score(X_eval, eval_labels)\n",
    "                db = davies_bouldin_score(X_eval, eval_labels)\n",
    "                ch = calinski_harabasz_score(X_eval, eval_labels)\n",
    "            else:\n",
    "                sil, db, ch = -1, 999, 0\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error at {params}: {e}\")\n",
    "            sil, db, ch, n_clusters = np.nan, np.nan, np.nan, 0\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "        \n",
    "        # 3. Save result\n",
    "        res = {\n",
    "            'timestamp': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            'model': model_name,\n",
    "            'params': str(params),\n",
    "            'silhouette': sil,\n",
    "            'db_index': db,\n",
    "            'ch_score': ch,\n",
    "            'n_clusters': n_clusters,\n",
    "            'time': elapsed\n",
    "        }\n",
    "        pd.DataFrame([res]).to_csv(output_file, mode='a', header=False, index=False)\n",
    "        \n",
    "        print(f\"[{i+1}/{len(grid)}] Clusters: {n_clusters} | Sil: {sil:.3f} | Params: {params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "302b317e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3. Define Fast Grids & Execute\n",
    "\n",
    "# K-MEANS GRID\n",
    "kmeans_grid = {\n",
    "    'n_clusters': range(2, 11),\n",
    "    'n_init': [5],        # Reduced from 10 for speed\n",
    "    'max_iter': [100],    # Reduced from 300 for speed\n",
    "    'random_state': [42]\n",
    "}\n",
    "\n",
    "# K-MEDOIDS (CLARA approach)\n",
    "# Note: We fit on a sample inside the loop for extreme speed\n",
    "kmedoids_grid = {\n",
    "    'n_clusters': range(2, 11),\n",
    "    'method': ['pam'],\n",
    "    'init': ['k-medoids++'],\n",
    "    'random_state': [42]\n",
    "}\n",
    "\n",
    "# DBSCAN GRID\n",
    "dbscan_grid = {\n",
    "    'eps': [0.05, 0.1, 0.15, 0.2, 0.3, 0.5],\n",
    "    'min_samples': [5, 10, 20, 50],\n",
    "    'n_jobs': [-1] # Use all cores\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88ee7738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execution\n",
    "# For K-Medoids, we pass a smaller training sample to mimic CLARA's speed\n",
    "X_train_small = resample(X_scaled, n_samples=10000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbcdc9e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Starting Search: K-Means\n",
      "[1/9] Clusters: 2 | Sil: 0.478 | Params: {'max_iter': 100, 'n_clusters': 2, 'n_init': 5, 'random_state': 42}\n",
      "[2/9] Clusters: 3 | Sil: 0.414 | Params: {'max_iter': 100, 'n_clusters': 3, 'n_init': 5, 'random_state': 42}\n",
      "[3/9] Clusters: 4 | Sil: 0.330 | Params: {'max_iter': 100, 'n_clusters': 4, 'n_init': 5, 'random_state': 42}\n",
      "[4/9] Clusters: 5 | Sil: 0.352 | Params: {'max_iter': 100, 'n_clusters': 5, 'n_init': 5, 'random_state': 42}\n",
      "[5/9] Clusters: 6 | Sil: 0.347 | Params: {'max_iter': 100, 'n_clusters': 6, 'n_init': 5, 'random_state': 42}\n",
      "[6/9] Clusters: 7 | Sil: 0.352 | Params: {'max_iter': 100, 'n_clusters': 7, 'n_init': 5, 'random_state': 42}\n",
      "[7/9] Clusters: 8 | Sil: 0.351 | Params: {'max_iter': 100, 'n_clusters': 8, 'n_init': 5, 'random_state': 42}\n",
      "[8/9] Clusters: 9 | Sil: 0.351 | Params: {'max_iter': 100, 'n_clusters': 9, 'n_init': 5, 'random_state': 42}\n",
      "[9/9] Clusters: 10 | Sil: 0.300 | Params: {'max_iter': 100, 'n_clusters': 10, 'n_init': 5, 'random_state': 42}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "run_clustering_search(KMeans, kmeans_grid, \"K-Means\", X_scaled, X_eval_sample, RESULTS_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0efef0df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Starting Search: K-Medoids\n",
      "[1/9] Clusters: 2 | Sil: 0.329 | Params: {'init': 'k-medoids++', 'method': 'pam', 'n_clusters': 2, 'random_state': 42}\n",
      "[2/9] Clusters: 3 | Sil: 0.411 | Params: {'init': 'k-medoids++', 'method': 'pam', 'n_clusters': 3, 'random_state': 42}\n",
      "[3/9] Clusters: 4 | Sil: 0.326 | Params: {'init': 'k-medoids++', 'method': 'pam', 'n_clusters': 4, 'random_state': 42}\n",
      "[4/9] Clusters: 5 | Sil: 0.348 | Params: {'init': 'k-medoids++', 'method': 'pam', 'n_clusters': 5, 'random_state': 42}\n",
      "[5/9] Clusters: 6 | Sil: 0.348 | Params: {'init': 'k-medoids++', 'method': 'pam', 'n_clusters': 6, 'random_state': 42}\n",
      "[6/9] Clusters: 7 | Sil: 0.322 | Params: {'init': 'k-medoids++', 'method': 'pam', 'n_clusters': 7, 'random_state': 42}\n",
      "[7/9] Clusters: 8 | Sil: 0.317 | Params: {'init': 'k-medoids++', 'method': 'pam', 'n_clusters': 8, 'random_state': 42}\n",
      "[8/9] Clusters: 9 | Sil: 0.272 | Params: {'init': 'k-medoids++', 'method': 'pam', 'n_clusters': 9, 'random_state': 42}\n",
      "[9/9] Clusters: 10 | Sil: 0.282 | Params: {'init': 'k-medoids++', 'method': 'pam', 'n_clusters': 10, 'random_state': 42}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "run_clustering_search(KMedoids, kmedoids_grid, \"K-Medoids\", X_train_small, X_eval_sample, RESULTS_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c2a5f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Starting Search: DBSCAN\n",
      "[1/24] Clusters: 83 | Sil: 0.040 | Params: {'eps': 0.05, 'min_samples': 5, 'n_jobs': -1}\n",
      "[2/24] Clusters: 41 | Sil: 0.015 | Params: {'eps': 0.05, 'min_samples': 10, 'n_jobs': -1}\n",
      "[3/24] Clusters: 19 | Sil: 0.003 | Params: {'eps': 0.05, 'min_samples': 20, 'n_jobs': -1}\n",
      "[4/24] Clusters: 6 | Sil: -0.172 | Params: {'eps': 0.05, 'min_samples': 50, 'n_jobs': -1}\n",
      "[5/24] Clusters: 66 | Sil: 0.167 | Params: {'eps': 0.1, 'min_samples': 5, 'n_jobs': -1}\n",
      "[6/24] Clusters: 37 | Sil: 0.178 | Params: {'eps': 0.1, 'min_samples': 10, 'n_jobs': -1}\n",
      "[7/24] Clusters: 26 | Sil: 0.138 | Params: {'eps': 0.1, 'min_samples': 20, 'n_jobs': -1}\n",
      "[8/24] Clusters: 7 | Sil: 0.122 | Params: {'eps': 0.1, 'min_samples': 50, 'n_jobs': -1}\n",
      "[9/24] Clusters: 58 | Sil: 0.216 | Params: {'eps': 0.15, 'min_samples': 5, 'n_jobs': -1}\n",
      "[10/24] Clusters: 35 | Sil: 0.205 | Params: {'eps': 0.15, 'min_samples': 10, 'n_jobs': -1}\n",
      "[11/24] Clusters: 21 | Sil: 0.221 | Params: {'eps': 0.15, 'min_samples': 20, 'n_jobs': -1}\n",
      "[12/24] Clusters: 8 | Sil: 0.189 | Params: {'eps': 0.15, 'min_samples': 50, 'n_jobs': -1}\n",
      "[13/24] Clusters: 34 | Sil: 0.218 | Params: {'eps': 0.2, 'min_samples': 5, 'n_jobs': -1}\n",
      "[14/24] Clusters: 24 | Sil: 0.226 | Params: {'eps': 0.2, 'min_samples': 10, 'n_jobs': -1}\n",
      "[15/24] Clusters: 19 | Sil: 0.186 | Params: {'eps': 0.2, 'min_samples': 20, 'n_jobs': -1}\n",
      "[16/24] Clusters: 9 | Sil: 0.202 | Params: {'eps': 0.2, 'min_samples': 50, 'n_jobs': -1}\n",
      "[17/24] Clusters: 7 | Sil: 0.359 | Params: {'eps': 0.3, 'min_samples': 5, 'n_jobs': -1}\n",
      "[18/24] Clusters: 7 | Sil: 0.358 | Params: {'eps': 0.3, 'min_samples': 10, 'n_jobs': -1}\n",
      "[19/24] Clusters: 6 | Sil: 0.366 | Params: {'eps': 0.3, 'min_samples': 20, 'n_jobs': -1}\n",
      "[20/24] Clusters: 5 | Sil: 0.287 | Params: {'eps': 0.3, 'min_samples': 50, 'n_jobs': -1}\n",
      "Error at {'eps': 0.5, 'min_samples': 5, 'n_jobs': -1}: bad allocation\n",
      "[21/24] Clusters: 0 | Sil: nan | Params: {'eps': 0.5, 'min_samples': 5, 'n_jobs': -1}\n",
      "Error at {'eps': 0.5, 'min_samples': 10, 'n_jobs': -1}: bad allocation\n",
      "[22/24] Clusters: 0 | Sil: nan | Params: {'eps': 0.5, 'min_samples': 10, 'n_jobs': -1}\n",
      "Error at {'eps': 0.5, 'min_samples': 20, 'n_jobs': -1}: bad allocation\n",
      "[23/24] Clusters: 0 | Sil: nan | Params: {'eps': 0.5, 'min_samples': 20, 'n_jobs': -1}\n",
      "Error at {'eps': 0.5, 'min_samples': 50, 'n_jobs': -1}: bad allocation\n",
      "[24/24] Clusters: 0 | Sil: nan | Params: {'eps': 0.5, 'min_samples': 50, 'n_jobs': -1}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "run_clustering_search(DBSCAN, dbscan_grid, \"DBSCAN\", X_scaled, X_eval_sample, RESULTS_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71356abf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6bb8a531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters Found (Minimizing Davies-Bouldin Index):\n",
      "--------------------------------------------------------------------------------\n",
      "        model  db_index  silhouette  n_clusters  \\\n",
      "11  K-Medoids  0.852171    0.411218           3   \n",
      "1     K-Means  0.933918    0.413515           3   \n",
      "24     DBSCAN  0.999089    0.177614          37   \n",
      "\n",
      "                                               params  \n",
      "11  {'init': 'k-medoids++', 'method': 'pam', 'n_cl...  \n",
      "1   {'max_iter': 100, 'n_clusters': 3, 'n_init': 5...  \n",
      "24      {'eps': 0.1, 'min_samples': 10, 'n_jobs': -1}  \n",
      "\n",
      "Best for K-Medoids:\n",
      "   • Davies-Bouldin Index: 0.8522\n",
      "   • Silhouette Score:     0.4112\n",
      "   • Number of Clusters:   3\n",
      "   • Optimal Parameters:    {'init': 'k-medoids++', 'method': 'pam', 'n_clusters': 3, 'random_state': 42}\n",
      "\n",
      "Best for K-Means:\n",
      "   • Davies-Bouldin Index: 0.9339\n",
      "   • Silhouette Score:     0.4135\n",
      "   • Number of Clusters:   3\n",
      "   • Optimal Parameters:    {'max_iter': 100, 'n_clusters': 3, 'n_init': 5, 'random_state': 42}\n",
      "\n",
      "Best for DBSCAN:\n",
      "   • Davies-Bouldin Index: 0.9991\n",
      "   • Silhouette Score:     0.1776\n",
      "   • Number of Clusters:   37\n",
      "   • Optimal Parameters:    {'eps': 0.1, 'min_samples': 10, 'n_jobs': -1}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## 4. View Best Results (Optimized for Davies-Bouldin)\n",
    "\n",
    "\n",
    "\n",
    "results = pd.read_csv(RESULTS_FILE)\n",
    "\n",
    "# IMPORTANT: We filter out cases where db_index is 999 (failed/single cluster) \n",
    "# and sort ASCENDING because for DB Index, lower is better.\n",
    "valid_results = results[results['db_index'] < 999]\n",
    "\n",
    "# Get the best (minimum) DB Index per model\n",
    "best = valid_results.sort_values('db_index', ascending=True).groupby('model').head(1)\n",
    "\n",
    "print(\"Best Parameters Found (Minimizing Davies-Bouldin Index):\")\n",
    "print(\"-\" * 80)\n",
    "# We include Silhouette just for comparison, but optimize for DB Index\n",
    "print(best[['model', 'db_index', 'silhouette', 'n_clusters', 'params']])\n",
    "\n",
    "# Detailed breakdown\n",
    "for index, row in best.iterrows():\n",
    "    print(f\"\\nBest for {row['model']}:\")\n",
    "    print(f\"   • Davies-Bouldin Index: {row['db_index']:.4f}\")\n",
    "    print(f\"   • Silhouette Score:     {row['silhouette']:.4f}\")\n",
    "    print(f\"   • Number of Clusters:   {row['n_clusters']}\")\n",
    "    print(f\"   • Optimal Parameters:    {row['params']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f4b1d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
